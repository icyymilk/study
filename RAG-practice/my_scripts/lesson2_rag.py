import numpy as np
from sentence_transformers import SentenceTransformer
from colorama import Fore, Style
import os 
import time
from pathlib import Path

def print_section(title):
    """Helper function to print section headers"""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{title}")
    print(f"{'='*80}{Style.RESET_ALL}\n")

def operation():
    
    
    examples = [
        "I love machine learning",
        "I enjoy artificial intelligence",
        "The weather is nice today"
    ]
    for i, ex in enumerate(examples, 1):
        print(f"  {i}. '{ex}'")

    sentences = [
        "I love cats",
        "I love dogs",
        "Cats and dogs"
    ]
    #å»é™¤é‡å¤çš„ï¼Œå¹¶æŒ‰å­—æ¯æ’åº
    vocab = sorted(set(' '.join(sentences).lower().split()))
    print(vocab)
    for sent in sentences:
        words = sent.lower().split()
        vector = [words.count(word) for word in vocab]
        print(f"'{sent}->{vector}'")

    print("\n   âš ï¸  Limitations:")
    print("      - Ignores word order: 'dog bites man' = 'man bites dog'")
    print("      - Ignores semantics: 'cat' and 'kitten' are unrelated")
    print("      - High dimensional (one dimension per word)\n")

    print("2. Modern Solution: Neural Embeddings")
    print("   - Pre-trained on massive text corpora")
    print("   - Capture semantic relationships")
    print("   - Fixed dimensions (e.g., 384, 768)")
    print("   - Understand context!\n")

    start_time = time.time()
    model = SentenceTransformer('all-MiniLM-L6-v2')
    load_time = time.time()-start_time
    print(f"Model loaded in {load_time:.2f}seconds\n")
    print(model.state_dict().keys())
    embeddings = model.encode(examples)
    print(f"{embeddings.shape}")
    print(f"  - {embeddings.shape[0]} sentences")
    print(f"  - {embeddings.shape[1]} dimensions each\n")

    print(f" {embeddings[0][:10]}\n")

    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(embeddings)

    print("Cosine similarities between sentences:")
    for i in range(len(sentences)):
        for j in range(i+1, len(sentences)):
            sim = similarities[i][j]
            print(f"  Sentence {i+1} â†” Sentence {j+1}: {sim:.4f}")
            print(f"    '{sentences[i]}'")
            print(f"    '{sentences[j]}'")
            print()

    print("ğŸ’¡ Notice: Sentences 1 and 2 have HIGH similarity (both about AI/ML)")
    print("   Sentences with weather have LOW similarity to AI/ML sentences!\n")

def deal_with_job_ads():
    corpus_path = Path("example_corpus")
    job_ads = {}
# ä»£ç åŠŸèƒ½ç®€çŸ­æ€»ç»“
# æ–‡ä»¶ç­›é€‰ä¸æ’åºï¼šåœ¨corpus_pathç›®å½•ä¸‹ï¼Œç­›é€‰å‡ºæ‰€æœ‰ä»¥ â€œjob_ad_â€ å¼€å¤´ã€ä»¥ â€œ.txtâ€ ç»“å°¾çš„æ–‡ä»¶ï¼Œå¹¶æŒ‰é¡ºåºæ’åºã€‚
# æ–‡ä»¶è¯»å–ä¸æ•°æ®å­˜å‚¨ï¼šé€ä¸ªæ‰“å¼€ç­›é€‰åçš„æ–‡ä»¶ï¼ˆä»¥ UTF-8 ç¼–ç è¯»å–ï¼‰ï¼Œå°†æ¯ä¸ªæ–‡ä»¶çš„å†…å®¹è¯»å–å‡ºæ¥ï¼Œä»¥ â€œæ–‡ä»¶åå‰ç¼€ï¼ˆå»é™¤æ‰©å±•åï¼Œå¦‚ job_ad_1ï¼‰â€ ä¸ºé”®ã€æ–‡ä»¶å†…å®¹ä¸ºå€¼ï¼Œå­˜å…¥job_adså­—å…¸ä¸­ã€‚
    for file_path in sorted(corpus_path.glob("job_ad_*.txt")):
        with open(file_path, 'r',encoding = 'utf-8') as f:
            job_ads[file_path.stem] = f.read()
    print(f"Loaded {len(job_ads)}job advertisement")

    for i, (name,content) in enumerate(list(job_ads.items())[:2],1):
        preview = content[:200].replace('\n',' ')
        print(f"{i}.{name}:")
        print(f"  {preview}...\n")
    
    print("Loading embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Create embeddings
    print("\nCreating embeddings for all job ads...")
    job_texts = list(job_ads.values())
    job_names = list(job_ads.keys())

    start_time = time.time()

    #éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„åµŒå…¥ä¸åŒäºGPTç±»å‹çš„LLMï¼Œä»–çš„åµŒå…¥æ˜¯é’ˆå¯¹å¥å­çš„ï¼Œæ¯ä¸ªå¥å­å¯¹åº”ä¸€ä¸ª384ç»´çš„å‘é‡ï¼Œæ¨¡å‹é€šè¿‡transformeræ¥æ•è·å…¶è¯­ä¹‰å†…æ¶µå’Œä¸Šä¸‹æ–‡ä¿¡æ¯
    embeddings = model.encode(job_texts,show_progress_bar=True)
    encode_time = time.time() - start_time

    print(
        f"\nâœ“ Created {len(embeddings)} embeddings in {encode_time:.2f} seconds")
    print(f"  Shape: {embeddings.shape}\n")


    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(embeddings)

    print("Most similar job ad pairs:")
    pairs = []
    for i in range(len(similarities)):
        for j in range(i+1,len(similarities)):
            pairs.append((i, j, similarities[i][j]))
    pairs.sort(key = lambda x:x[2],reverse=True)
    for i,j,sim in pairs[:3]:
        print(f"\n {job_names[i]} <-> {job_names[j]}")
        print(f"Similarity: {sim:.4f}")
    print("\nğŸ’¡ High similarity means these jobs require similar skills/experience!\n")

# å®šä¹‰äº†è¯­æ–™åº“è·¯å¾„ corpus_pathï¼ŒæŒ‡å‘åä¸º "example_corpus" çš„ç›®å½•ï¼›
# åˆå§‹åŒ–ç©ºå­—å…¸ job_adsï¼Œç”¨äºå­˜å‚¨èŒä½æ‹›è˜ä¿¡æ¯ï¼›
# éå† corpus_path ç›®å½•ä¸‹ã€æŒ‰æ–‡ä»¶åæ’åºçš„æ‰€æœ‰ä»¥ "job_ad_" å¼€å¤´ä¸”åç¼€
# ä¸º ".txt" çš„æ–‡ä»¶ï¼ˆå³èŒä½å¹¿å‘Šæ–‡æœ¬æ–‡ä»¶ï¼‰ï¼Œæ–‡ä»¶è·¯å¾„æš‚å­˜äº file_path å˜é‡ï¼ˆå½“å‰ä»£ç æœªä½“ç°åç»­æ–‡ä»¶å†…å®¹å¤„ç†é€»è¾‘ï¼‰
def operation_2():
    
    corpus_path =Path("example_corpus")
    job_ads = {}
    for file_path in sorted(corpus_path.glob("job_ad_*.txt")):
        with open(file_path,'r',encoding='utf-8') as f:
            job_ads[file_path.stem] = f.read()
    model = SentenceTransformer('all-MiniLM-L6-v2')
    job_names = list(job_ads.keys())
    job_texts = list(job_ads.values())
    embeddings = model.encode(job_texts,show_progress_bar=True)
    print(f"Create {len(embeddings)} embeddings({embeddings.shape[1]}dimensions)")

    def search(query,top_k=3):
        query_embeddings = model.encode(query,show_progress_bar=True)
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity([query_embeddings],embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = []
        # è¿™é‡Œæ˜¾å¼ä¼ å…¥ç¬¬äºŒä¸ªå‚æ•° 1ï¼Œå¼ºåˆ¶è®©æšä¸¾çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆæœ€ç›¸ä¼¼çš„ç»“æœï¼‰å¯¹åº” rank=1ï¼Œå› æ­¤æ’åä» 1 å¼€å§‹ã€‚
        # äºŒã€ä¸šåŠ¡å±‚é¢çš„åˆç†æ€§ï¼ˆä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼‰
        # è¿™æ˜¯æ’åç±»åŠŸèƒ½çš„é€šç”¨è®¾è®¡ä¹ æƒ¯ï¼šå¯¹ç”¨æˆ· / ä¸šåŠ¡ä¾§æ¥è¯´ï¼Œã€Œç¬¬ 1 åã€ç¬¬ 2 åã€ç¬¬ 3 åã€æ˜¯ç¬¦åˆäººç±»è®¤çŸ¥çš„è¡¨è¿°ï¼Œæ²¡äººä¼šè¯´ã€Œç¬¬ 0 åã€ï¼›
        # ä½ çš„å‡½æ•°æ˜¯ searchï¼ˆæœç´¢ï¼‰åŠŸèƒ½ï¼Œè¿”å›ã€Œtop_k=3ã€çš„ç»“æœï¼Œç”¨ 1/2/3 æ ‡æ³¨æ’åï¼Œæ¯” 0/1/2 æ›´ç›´è§‚ã€æ›´ç¬¦åˆä½¿ç”¨ä¹ æƒ¯
        for rank, idx in enumerate(top_indices,1):
            results.append({
                'rank':rank,
                'documents':job_names[idx],
                #æ ¼å¼å¤„ç†ï¼šå°†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦ï¼ˆ\nï¼‰æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œç»Ÿä¸€æ–‡æœ¬æ˜¾ç¤ºæ ¼å¼ï¼Œæ¶ˆé™¤æ¢è¡Œå¯¼è‡´çš„æ ¼å¼æ··ä¹±
                'texts':job_texts[idx][:150].replace('\n',' '),
                'similarties': similarities[idx]
            })
        return results

    queries = [
        "Python developer with machine learning experience",
        "Web developer position",
        "Software engineer role"
    ]

    for query in queries:
        results = search(query, top_k=3)

        print(f"Results for: '{query}'")
        print("-" * 80)
        for r in results:
            print(f"\n{r['rank']}. {r['document']}")
            print(f"   Similarity: {r['similarity']:.4f}")
            print(f"   Texts: {r['texts']}...")
        print()

###ä¼ ç»Ÿæ–¹æ³•ï¼šå…³é”®è¯é…å¯¹ï¼Œæ ¹æ®é…å¯¹æ•°æ’åºã€‚ç°ä»£æ–¹æ³•ï¼šè¯­ä¹‰é…å¯¹ï¼Œè¯­ä¹‰é€šè¿‡è®¡ç®—transformerè½¬æ¢çš„åµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥æ£€ç´¢
### æ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—è¯­ä¹‰æ£€ç´¢å¯ä»¥ç†è§£åŒä¹‰è¯ä»¥åŠç›¸å…³æ¦‚å¿µ
###âœ“ Semantic search: Finds 'software engineer' when searching 'developer'"
###âœ“ Semantic search: Finds 'software engineer' when searching 'developer'"



if __name__ == "__main__":
    operation()
    deal_with_job_ads()